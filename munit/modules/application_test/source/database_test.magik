#% text_encoding = iso8859_1
_package sw

_pragma(classify_level=debug)
##
## Top level test_case class providing infrastructure for tests
## that require an application to be running.
##
## This class provides a framework for writing tests that
## require an database, but not an application, to be running
## by implementing the following behaviour:
##
## * set_up() 
##   * ensure that there is a database
##   * ensure that the database is writable 
##     * It creates an in-memory scratchpad in any
##       datasets (as specified by dataset_names) in which the test
##       will make any changes.  
## * tear_down() 
##   * Remove the scratchpad so the dataset reverts to its original state. 
##
## Subclasses of database_test should implement the following methods:
##
## * writable_dataset_names - This should return the names of any datasets that need to be writable. 
##
## database_test provides the following API methods:
##
## * dataset( ds_name ) - This returns the dataset identified by ds_name  
##
def_slotted_exemplar( :database_test,
        {
		{ :properties,   _unset }
	},
        { :test_case, :database_system_test_mixin } )
$


_pragma(classify_level=advanced, usage={subclassable})
## Can be set to :vmsql in order to create datasets in Postgres.
database_test.define_shared_variable(:database_storage_mode, :vmds, :private)
$

_pragma(classify_level=advanced, usage={subclassable})
## Controls whether the VMSQL dump is to be imported as part of the
## one_time_set_up().  Ignored if import_vmsql_before_each_test? is true.  When
## both import_vmsql_before_suite? and import_vmsql_before_each_test is set to
## false then the dump will not be imported from Magik at all.
database_test.define_shared_variable(:import_vmsql_before_suite?, _false, :private)
$

_pragma(classify_level=advanced, usage={subclassable})
## When this is set to false the postgres dump will be loaded in
## one_time_set_up.  When set to true the Postgres dump will be loaded before
## each test, i.e. from the set_up.
database_test.define_shared_variable(:import_vmsql_before_each_test?, _false, :private)
$

_pragma(classify_level=debug)
database_test.define_shared_variable(:vmds_database_path,
        ##
        ## The location of the cambridge_db to use.
        ##
        "cbgsun11:/swdev/databases/dev_ds_500/ds/ds_admin", :public)
$

_pragma(classify_level=debug)
database_test.define_shared_variable(:vmsql_database_path,
        ##
        ## The location of the cambridge_db to use for VMSQL.
        ##
	        "cbgsun11:/swdev/databases/dev_ds_500_vmsql/ds/ds_admin", :public)
	
$


_pragma(classify_level=debug)
database_test.define_shared_variable(:concurrency_mode,
        ##
        ## The concurrecny_mode used to reference the database. This
        ## may be :single_user or :multi_user. The default is :multi_user.
        ##
        :multi_user, :public)
$


_pragma(classify_level=debug)
## This shared variable indicates if a database was opened by
## this test.  If so, it is closed in the one_time_tear_down()
## method. 
database_test.define_shared_variable( :database_opened?, _false, :private )
$


_pragma(classify_level=debug)
## This shared constant indicates if the database should be
## closed.  If true, it is closed in the one_time_tear_down()
## method regardless whether it was opened by this test.
database_test.define_shared_variable( :force_close?, _true, :private )
$

_pragma(classify_level=debug)
## Cambridge_db dump file name
database_test.define_shared_constant( :dump_file_name, "camb_gis.dump", :private )
$



database_test.method_table.resolve_conflicts( :test_case )
$

_pragma(classify_level=debug)
_method database_test.properties
	## 
	##

	>> .properties
	
_endmethod
$

_pragma(classify_level=debug)
_method database_test.database_path
	## 
	##

	>> _if _self.database_storage_mode _is :vmsql
		_then
			>> _self.vmsql_database_path
		_else
			>> _self.vmds_database_path
		_endif
_endmethod
$


_pragma(classify_level=debug, usage={subclassable})
_method database_test.writable_dataset_names
	## 
	## A vector containing the names of any datasets that need to
	## be writable.
	##
        >> {}
_endmethod 
$
_pragma(classify_level=debug, usage={subclassable})
## nslots when opening world for dataset clone
database_test.define_shared_variable(:clone_nslots, 1000, :public )
$
_pragma(classify_level=debug, usage={subclassable})
_method database_test.dataset_clones
	## Number of database clones on which test will run
	## concurrently alongside when running on main dataset followed
	## by boolean indicating whether clones should be writable
	## See also run_with_clones() API.
	>> 0, _false 
_endmethod
$
database_test.define_shared_variable(:clones, property_list.new(), :private)
$

_pragma(classify_level=debug, usage={subclassable})
_private _method database_test.dataset_clones?
	## 
	## 
	>> _self.dataset_clones > 0 _andif 
		_not _self.writable_dataset_names.empty? _andif
		_self.database_opened?
_endmethod
$
_pragma(classify_level=restricted,topic=unit_testing)
_private _method database_test.writable_dataset_clones?
	## 
	## 
	_return _self.dataset_clones? _andif (_allresults _self.dataset_clones)[2]
_endmethod
$

_pragma(classify_level=restricted,topic=unit_testing)
_private _method database_test.make_dataset_clones()
	## 
	##
	_dynamic !output!,!error!,!terminal!
	_if _not _self.dataset_clones?	_then _return _endif

	worlds<< simple_vector.new(_self.dataset_clones)
	_for i _over 1.upto(_self.dataset_clones)
	_loop
		worlds[i]<< ds_world.new(_self.clone_nslots)
	_endloop
	_for ds_name _over _self.writable_dataset_names.fast_elements()
	_loop
		write("Clonning ",_self.dataset_clones," times database: ",ds_name)
		_block
			# Suppress witterage when creating clone
			_handling information _with procedure 
			_handling warning _with procedure
			!error!<< !terminal!<< !output!<< internal_text_output_stream.new()
			
			_if (v<< _self.dataset(ds_name)) _is _unset _then _continue _endif
			_self.clones[ds_name]<< simple_vector.new(_self.dataset_clones)
			cont<<0
			_for i, a_world _over worlds.fast_keys_and_elements()
			_loop
				cont+<<1
				_if v.vmds?
				_then
					_self.clones[ds_name][i]<< clone<< gis_ds_view.new(:write, :searchpath, v.searchpath)
				_else
					dataset_name << ds_name+:|_|+cont.write_string.as_symbol()
					_self.clones[ds_name][i]<< clone<< gis_ds_view.new(:write,
											   :concurrency_mode, :vmsql,
											   :searchpath, v.searchpath,
											   :name, dataset_name)
					_self.import_vmsql_dump(dataset_name,_self.dump_file_name)
				_endif

				_for f _over v.files.fast_elements()
				_loop
					clone.add_file(f.name, :world, a_world )
				_endloop
				clone.init_view()
				clone.goto_top_alternative()
			_endloop
		_endblock
	_endloop 
_endmethod
$

_pragma(classify_level=restricted,topic=unit_testing)
_private _method database_test.given_writable_dataset_clones()
	## 
	##
	_if _not _self.writable_dataset_clones?	_then _return _endif
	# Create a scratchpad for each dataset clone to hold any changes
	_for ds_name _over _self.writable_dataset_names.fast_elements()
	_loop
		_if (clones << _self.clones[ds_name]) _is _unset _then _continue _endif 
		_for i,clone _over clones.fast_keys_and_elements()
		_loop
			clone.goto_top_alternative(:readonly)
			_if clone.vmds?
			_then
				clone.create_scratchpad( write_string("memory::ds",i) )
			_else
				count << 1
				_while clone.has_alternative?(count.write_string)
				_loop
					count +<<1
				_endloop
				clone.spawn(count.write_string)
				clone.down(count.write_string,:write)
			_endif
		_endloop
	_endloop	
_endmethod
$

_pragma(classify_level=restricted,topic=unit_testing)
_private _method database_test.close_dataset_clones()
	## 
	##
	_if _not _self.writable_dataset_clones?	_then _return _endif
	_for ds_name _over _self.writable_dataset_names.fast_elements()
	_loop
		_if (clones << _self.clones[ds_name]) _is _unset _then _continue _endif 
		_for clone _over clones.fast_elements()
		_loop
			clone.rollback()
			clone.up()
		_endloop
	_endloop
_endmethod
$
_pragma(classify_level=restricted,topic=unit_testing)
_method database_test.run_with_clones(ds_name, meth_name, _gather parameters)
	## Runs method METH_NAME for DS_NAME and all its clones in parallel
	## The method METH_NAME is expected to have parameters view and PARAMETERS.
	## Method is run in parallel on the view DS_VIEW and its clones.
	_if _self.dataset_clones < 1 _orif
	    (clones<< _self.clones[ds_name]) _is _unset
	_then
		_return _self.perform(meth_name, _self.dataset(ds_name), _scatter parameters)
	_endif

	_local aq<< atomic_queue.new(_self.dataset_clones + 1)
	_local a_test<< _self 
	_local run_for_dataset<< _proc @run_for_dataset(view, meth_name, _gather parameters)
					 _import aq, a_test
					 _dynamic !current_dsview!
					 !current_dsview!<< view
					 _protect
						 a_test.perform(meth_name, view, _scatter parameters)
					 _protection
						 aq.put(1)
					 _endprotect
				 _endproc
	
	_local started<< 0
	_protect
		started+<< 1
		run_for_dataset.fork(_self.dataset(ds_name), meth_name, _scatter parameters)
		_for clone _over clones.fast_elements()
		_loop
			started+<< 1
			run_for_dataset.fork(clone, meth_name, _scatter parameters)
		_endloop
	_protection
		write("Waiting for ",started," tests ",meth_name," to finish.") 
		id << 0
		_if started > 0
		_then 
			_loop
				# waiting 30 - 60 sec max
				_if aq.get(60 * 1000 * (started - id/2) / started) _is _unset _then _leave _endif 
				id+<< 1
				_if id >= started _then _leave _endif 
			_endloop
		_endif
	_endprotect	
_endmethod
$


_pragma(classify_level=debug, usage={subclassable})
_method database_test.other_dataset_names
        ##
        ## The names of any datasets which aren't in the normal SOC (eg case)
        ##
        >> {}
_endmethod 
$

_pragma(classify_level=debug)
_method database_test.one_time_set_up()
	##
	## This gets called before first test in this test class and
	## ensures that the correct database is available. 
	## It also creates a scratchpad to hold any transient data.
        ##
        ## Subclass implemetations should always call _super.
	##
        _super.one_time_set_up()
        _self.check_correct_database_open()
	_self.make_dataset_clones()
_endmethod
$
_pragma(classify_level=debug)
_private _method database_test.discard_clones()
	## 
	## 
	_if _not _self.dataset_clones? _orif _self.clones.empty? _then _return _endif 
	worlds<< set.new()
	_for clone _over _self.clones.an_element().fast_elements()
	_loop
		worlds.add(clone.files.an_element().world)
	_endloop
	_for clones _over _self.clones.fast_elements()
	_loop
		_for clone _over clones.fast_elements()
		_loop
			clone.discard()
		_endloop
	_endloop
	_for a_world _over worlds.fast_elements()
	_loop
		a_world.close()
	_endloop
	_self.clones.empty()
_endmethod
$

_pragma(classify_level=debug)
_method database_test.one_time_tear_down()
	##
	## This gets called before first test in next test class and
	## ensures that the database opened in this test is discarded.
	##
        ## Subclass implemetations should always call _super.
	##
        _super.one_time_tear_down()
	_if _self.database_opened? _orif
	    _self.force_close?
	_then 
		_self.discard_clones()
		gis_program_manager.reinitialise()
	_endif
_endmethod
$

_pragma(classify_level=debug)
_method database_test.set_up()
	##
	## This gets called before each test and ensures that the
	## correct database is available. 
	## It also creates a scratchpad to hold any transient data.
        ##
        ## Subclass implemetations should always call _super.
	##
        _super.set_up()
	.properties << property_list.new()
	_self.set_up_vmsql_db()
	_self.given_writable_datasets( _self.writable_dataset_names )
	_self.given_writable_dataset_clones()
_endmethod
$
_pragma(classify_level=debug)
_method database_test.run_bare()
	## 
	## 
	#
	# The Cambridge DB exemplars are declared in the user package,
	# so this must be current when we open the database
	#
	_dynamic !current_package! << !current_package!.all_packages[:user]
	_super.run_bare()
_endmethod
$

_private _method database_test.check_correct_database_open()
        ## 
        ## If there is no database open, then opens the nominated
        ## one. If one is open but it isn't the correct one (the ace
        ## path is incorrect) then closes that and opens the correct
        ## one.
        ##
	
	_self.database_opened? << _false
	
	_if _not ds_environment.c_is_initialised? _orif
		gis_program_manager.ace_top_view _is _unset _orif
		gis_program_manager.ace_top_view.searchpath.at_using_default("", 1) <> _self.database_path _orif
		ds_environment.concurrency_mode <> _self.concurrency_mode
	_then 
		# Suppress witterage
		_handling information _with procedure 
		_dynamic !output!
		_if !msf_debug?! _isnt _true
		_then
			!output!<< internal_text_output_stream.new()
		_endif

		ds_environment.concurrency_mode << _self.concurrency_mode
		smallworld_product.set_startup_option(:authorisation, :singleuser)
		smallworld_product.set_startup_option(:login, "root")
		interactive?<< smallworld_product.get_option(:interactive?) _is _true 
		_if interactive?
		_then 
			smallworld_product.set_startup_option(:interactive?, _false)
		_endif
		gis_program_manager.reinitialise()
		_self.prep_vmsql_db(:gis)
		open_database({_self.database_path})
		_if interactive?
		_then 
			smallworld_product.set_startup_option(:interactive?, _true)
		_endif
		_self.database_opened? << _true
	_endif
	_self.open_other_datasets()
_endmethod
$


_pragma(classify_level=debug)
_method database_test.tear_down()
	##
	## This gets called after each test and deletes the scratchpad.
	##

	_super( test_case ).tear_down()
	_self.close_datasets()
	_self.close_dataset_clones()
_endmethod
$

_pragma(classify_level=debug)
_method database_test.open_other_datasets()
        ##
        ## Open any datasets used by the case tool
        ##
        _for name _over _self.other_dataset_names.elements()
        _loop 
                _if gis_program_manager.databases[name] _is _unset
                _then gis_program_manager.open_datasets(name)
                _endif 
        _endloop 
_endmethod 
$
_pragma(classify_level=restricted,topic=MUnit)
_private _method database_test.wait_for_threads_to_finish( task_owner )
	##
	## Wait for all the tasks on TASK_OWNER to finish.
	##
	## Wait for 1 second and check if TASK_OWNER.thread_pool is
	## empty.
	##
	## Implementation taken from application_system_test_mixin.

	_loop
		_if task_owner.task_runner().thread_pool.empty? _then _leave _endif
		_thisthread.sleep( 1000 )
	_endloop 
	
_endmethod
$

_private _method database_test.set_up_vmsql_db()
	##
	## Called from set_up() to import VMSQL db when appropriate.
	##
	_if _self.database_storage_mode _is :vmsql _andif 
		_self.import_vmsql_before_each_test? _is _true
	_then
		_for soc _over gis_program_manager.spatial_object_controllers()
		_loop
			_for dsm _over soc.dataset_managers()
			_loop
				_if dsm.is_kind_of?(vmsql_manager_mixin) _andif dsm.dataset_open?()
				_then
					dsm.close()
					_self.import_vmsql_dump(dsm.name)	
					dsm.open(dsm.get_connect_spec())
				_endif
			_endloop
		_endloop
	_endif
_endmethod
$

_private _method database_test.prep_vmsql_db(dataset_name)
	##
	## Check the VMSQL database before opening the database.  Called from
	## one_time_set_up before we do open_database.
	##
	_if _self.database_storage_mode _isnt :vmsql _orif
		_self.import_vmsql_before_suite? _isnt _true
	_then
		_return
	_endif
	_self.import_vmsql_dump(dataset_name)
_endmethod
$

_private _method database_test.import_vmsql_dump(dataset_name,_optional dump_name)
	##
	## Import a a dump file named camdb_DATASET_NAME.dump from the 
	##

	_self.recreate_vmsql_database(dataset_name)
	pghost << system.getenv("SW_PGHOST")
	pgdb << dataset_name.write_string	
	dump_name << dump_name.default(_self.dump_file_name)
	container_name << write_string("pg-", system.getenv("SW_PGPORT"))
	cmd << rope.new()
	cmd.add_last(_self.ant_executable)
	cmd.add_last("-f")
	cmd.add_last(_self.vmsql_ant_build_xml)
	cmd.add_last(write_string("-D", "sw.pghost=", pghost))
	cmd.add_last(write_string("-D", "postgres.container=", container_name))
	cmd.add_last(write_string("-D", "postgres.database=", pgdb))
	cmd.add_last(write_string("-D", "dumpfile=",dump_name))
	cmd.add_last("import-vmsql-dump")
	system.do_command(cmd.join_as_strings(" "))
_endmethod
$

_private _method database_test.vmsql_ant_build_xml
	##
	## Import a a dump file named camdb_DATASET_NAME.dump from the 
	##
	product_path << sw_module_manager.module(_self.module_name).product.directory
	ant_build_file << system.canonicalise(system.pathname_down(product_path, "..", "..", "..", "build_tools", "ant", "vmds_cloud_common.xml"))
	_if _not system.file_exists?(ant_build_file)
	_then
		condition.raise(:warning, :string, write_string("ANT build file does not exist: ", ant_build_file))
	_endif
	_return ant_build_file
_endmethod
$

_private _method database_test.ant_executable
	##
	## Returns the full path to the ant executable.  Depends on the ANT_HOME
	## environment variable being set.  For Linux the ant executalbe will just be
	## ant, for Windows it will be ant.bat.  Raises warnings when ANT_HOME is not
	## set or when the ant executable is not found where the code expects it to be.
	##
	ant_home << system.getenv("ANT_HOME")
	_if ant_home _is _unset
	_then
		condition.raise(:warning, :string, "ANT_HOME is not set!")
	_endif
		
	ant_exe_ext << ""
	_if system.os_name _is :windows 
	_then
		ant_exe_ext << ".bat"
	_endif

	ant_exe << 	system.pathname_down(ant_home, "bin", write_string("ant", ant_exe_ext))
	_if _not system.file_exists?(ant_exe)
	_then
		condition.raise(:warning, :string, write_string("ANT executable does not exist: ", ant_exe))
	_endif
	_return ant_exe
_endmethod
$

